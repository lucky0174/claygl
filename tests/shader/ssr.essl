// http://www.kode80.com/blog/2015/03/11/screen-space-reflections-in-unity-5/
// http://casual-effects.blogspot.jp/2014/08/screen-space-ray-tracing.html
@export ssr.fragment

#define MAX_ITERATION 20;
#define MAX_BINARY_SEARCH_ITERATION 5;

uniform sampler2D colorTex;
uniform sampler2D normalTex;
uniform sampler2D depthTex;
uniform sampler2D backDepthTex;

uniform mat4 projection;
uniform mat4 projectionInv;
uniform mat4 viewInverseTranspose;

uniform float maxRayDistance: 2;

uniform float pixelStride: 16;
uniform float pixelStrideZCutoff: 10; // ray origin Z at this distance will have a pixel stride of 1.0

uniform float screenEdgeFadeStart: 0.9; // distance to screen edge that ray hits will start to fade (0.0 -> 1.0)

uniform float eyeFadeStart : 0.4; // ray direction's Z that ray hits will start to fade (0.0 -> 1.0)
uniform float eyeFadeEnd: 0.8; // ray direction's Z that ray hits will be cut (0.0 -> 1.0)


uniform float nearZ;
uniform vec2 viewportSize;

uniform float maxMipmapLevel: 5;

varying vec2 v_Texcoord;

#ifdef DEPTH_DECODE
@import qtek.util.decode_float
#endif

float fetchDepth(sampler2D depthTexture, vec2 uv)
{
    vec4 depthTexel = texture2D(depthTexture, uv);
#ifdef DEPTH_DECODE
    return decodeFloat(depthTexel) * 2.0 - 1.0;
#else
    return depthTexel.r * 2.0 - 1.0;
#endif
}

float linearDepth(float depth)
{
    return projection[3][2] / (depth * projection[2][3] - projection[2][2]);
}

bool rayIntersectDepth(float rayZNear, float rayZFar, vec2 hitPixel)
{
    float cameraZ = linearDepth(fetchDepth(depthTex, hitPixel));
    float cameraBackZ = linearDepth(fetchDepth(backDepthTex, hitPixel));
    // Cross z
    return rayZFar <= cameraZ && rayZNear >= cameraBackZ;
}

// Trace a ray in screenspace from rayOrigin (in camera space) pointing in rayDir (in camera space)
//
// With perspective correct interpolation
//
// Returns true if the ray hits a pixel in the depth buffer
// and outputs the hitPixel (in UV space), the hitPoint (in camera space) and the number
// of iterations it took to get there.
//
// Based on Morgan McGuire & Mike Mara's GLSL implementation:
// http://casual-effects.blogspot.com/2014/08/screen-space-ray-tracing.html

bool traceScreenSpaceRay(
    vec3 rayOrigin, vec3 rayDir, float jitter,
    out vec2 hitPixel, out vec3 hitPoint, out float iterationCount
)
{
    // Clip to the near plane
    float rayLength = ((rayOrigin.z + rayDir.z * maxRayDistance) > -nearZ)
        ? (-nearZ - rayOrigin.z) / rayDir.z : maxRayDistance;

    vec3 rayEnd = rayOrigin + rayDir * rayLength;

    // Project into homogeneous clip space
    vec4 H0 = projection * vec4(rayOrigin, 1.0);
    vec4 H1 = projection * vec4(rayEnd, 1.0);

    float k0 = 1.0 / H0.w, k1 = 1.0 / H1.w;

    // The interpolated homogeneous version of the camera space points
    vec3 Q0 = rayOrigin * k0, Q1 = rayEnd * k1;

    // Screen space endpoints
    // PENDING viewportSize ?
    vec2 P0 = (H0.xy * k0 * 0.5 + 0.5) * viewportSize;
    vec2 P1 = (H1.xy * k1 * 0.5 + 0.5) * viewportSize;

    // If the line is degenerate, make it cover at least one pixel to avoid handling
    // zero-pixel extent as a special case later
    P1 += dot(P1 - P0, P1 - P0) < 0.0001 ? 0.01 : 0.0;
    vec2 delta = P1 - P0;

    // Permute so that the primary iteration is in x to collapse
    // all quadrant-specific DDA case later
    bool permute = false;
    if (abs(delta.x) < abs(delta.y)) {
        // More vertical line
        permute = true;
        delta = delta.yx;
        P0 = P0.yx;
        P1 = P1.yx;
    }
    float stepDir = sign(delta.x);
    float invdx = stepDir / delta.x;

    // Track the derivatives of Q and K
    vec3 dQ = (Q1 - Q0) * invdx;
    float dk = (k1 - k0) * invdx;

    vec2 dP = vec2(stepDir, delta.y * invdx);

    // Calculate pixel stride based on distance of ray origin from camera.
    // Since perspective means distant objects will be smaller in screen space
    // we can use this to have higher quality reflections for far away objects
    // while still using a large pixel stride for near objects (and increase performance)
    // this also helps mitigate artifacts on distant reflections when we use a large
    // pixel stride.
    float strideScaler = 1.0 - min(1.0, -rayOrigin.z / pixelStrideZCutoff);
    float pixStride = 1.0 + strideScaler * pixelStride;

    // Scale derivatives by the desired pixel stride and the offset the starting values by the jitter fraction
    dP *= pixStride; dQ *= pixStride; dk *= pixStride;
    P0 += dP * jitter; Q0 += dQ * jitter; k0 += dk * jitter;

    float rayZNear = rayOrigin.z;
    float rayZFar = rayOrigin.z;
    // Track ray step and derivatives in a vec4 to parallelize
    vec4 pqk = vec4(P0, Q0.z, k0);
    vec4 dPQK = vec4(dP, dQ.z, dk);

    bool intersect = false;

    vec2 texelSize = 1.0 / viewportSize;

    iterationCount = 0.0;
    for (int i = 0; i < MAX_ITERATION; i++)
    {
        pqk += dPQK;

        rayZNear = rayZFar;
        rayZFar = (dPQK.z * 0.5 + pqk.z) / (dPQK.w * 0.5 + pqk.w);
        // Swap if bigger
        if (rayZFar > rayZNear)
        {
            float t = rayZFar; rayZFar = rayZNear; rayZNear = t;
        }
        hitPixel = permute ? pqk.yx : pqk.xy;
        hitPixel *= texelSize;

        intersect = rayIntersectDepth(rayZNear, rayZFar, hitPixel);

        iterationCount += 1.0;

        // PENDING Right on all platforms?
        if (intersect) {
            break;
        }
    }

    // Binary search refinement
    if (pixelStride > 1.0 && intersect)
    {
        pqk -= dPQK;
        dPQK /= pixelStride;

        float originalStride = pixelStride * 0.5;
        float stride = originalStride;

        rayZNear = pqk.z / pqk.w;
        rayZFar = rayZNear;

        for (int j = 0; j < MAX_BINARY_SEARCH_ITERATION; j++)
        {
            pqk += dPQK * stride;
            rayZNear = rayZFar;
            rayZFar = (dPQK.z * -0.5 + pqk.z) / (dPQK.w * -0.5 + pqk.w);
            // Swap if bigger
            if (rayZFar > rayZNear)
            {
                float t = rayZFar; rayZFar = rayZNear; rayZNear = t;
            }
            hitPixel = permute ? pqk.yx : pqk.xy;
            hitPixel *= texelSize;

            originalStride *= 0.5;
            stride = rayIntersectDepth(rayZNear, rayZFar, hitPixel) ? -originalStride : originalStride;
        }
    }

    Q0.xy += dQ.xy * iterationCount;
    Q0.z = pqk.z;
    hitPoint = Q0 / pqk.w;

    return intersect;
}

float calculateAlpha(
    float iterationCount, float reflectivity,
    vec2 hitPixel, vec3 hitPoint, float dist, vec3 rayDir
)
{
    float alpha = min(1.0, reflectivity + 0.1);
    // Fade ray hits that approach the maximum iterations
    alpha *= 1.0 - (iterationCount / float(MAX_ITERATION));
    // Fade ray hits that approach the screen edge
    vec2 hitPixelNDC = hitPixel * 2.0 - 1.0;
    float maxDimension = min(1.0, max(abs(hitPixelNDC.x), abs(hitPixelNDC.y)));
    alpha *= 1.0 - max(0.0, maxDimension - screenEdgeFadeStart) / (1.0 - screenEdgeFadeStart);

    // Fade ray hits base on how much they face the camera
    float _eyeFadeStart = eyeFadeStart;
    float _eyeFadeEnd = eyeFadeEnd;
    if (_eyeFadeStart > _eyeFadeEnd) {
        float tmp = _eyeFadeEnd;
        _eyeFadeEnd = _eyeFadeStart;
        _eyeFadeStart = tmp;
    }

    float eyeDir = clamp(rayDir.z, _eyeFadeStart, _eyeFadeEnd);
    alpha *= 1.0 - (eyeDir - _eyeFadeStart) / (_eyeFadeEnd - _eyeFadeStart);

    // Fade ray hits based on distance from ray origin
    alpha *= 1.0 - clamp(dist / maxRayDistance, 0.0, 1.0);

    return alpha;
}

@import qtek.util.rgbm

void main()
{
    vec4 normalAndGloss = texture2D(normalTex, v_Texcoord);

    // Is empty
    if (dot(normalAndGloss.rgb, vec3(1.0)) == 0.0) {
        discard;
    }

    vec3 N = normalAndGloss.rgb * 2.0 - 1.0;
    N = (viewInverseTranspose * vec4(N, 0.0)).xyz;

    float g = normalAndGloss.a;
    // Position in view
    vec4 projectedPos = vec4(v_Texcoord * 2.0 - 1.0, fetchDepth(depthTex, v_Texcoord), 1.0);
    vec4 pos = projectionInv * projectedPos;
    vec3 rayOrigin = pos.xyz / pos.w;

    vec3 rayDir = normalize(reflect(normalize(rayOrigin), normalize(N)));
    vec2 hitPixel;
    vec3 hitPoint;
    float iterationCount;

    vec2 uv2 = v_Texcoord * viewportSize;
    float jitter = fract((uv2.x + uv2.y) * 0.25);

    bool intersect = traceScreenSpaceRay(rayOrigin, rayDir, jitter, hitPixel, hitPoint, iterationCount);

    float dist = distance(rayOrigin, hitPoint);

    float alpha = calculateAlpha(iterationCount, g, hitPixel, hitPoint, dist, rayDir) * float(intersect);
    hitPixel = mix(v_Texcoord, hitPixel, float(intersect));

    vec4 color = decodeHDR(texture2DLodEXT(colorTex, hitPixel, clamp(dist / maxRayDistance, 0.0, 1.0) * maxMipmapLevel));
    gl_FragColor = encodeHDR(vec4(color.rgb * alpha, color.a));

    if (!intersect) {
        discard;
    }
}
@end

// https://bartwronski.com/2014/03/23/gdc-follow-up-screenspace-reflections-filtering-and-up-sampling/
@export ssr.blur_h

uniform sampler2D colorTex;
uniform sampler2D normalTex;

varying vec2 v_Texcoord;

uniform vec2 textureSize;
uniform float blurSize : 1.5;

@import qtek.util.rgbm

void main()
{
    float g = texture2D(normalTex, v_Texcoord).a;
    // Add 0.3 bias to filling holes from missed rays.
    float off = (clamp(1.0 - g, 0.0, 1.0) + 0.3) / textureSize.x * blurSize;
    vec2 coord = v_Texcoord;

    vec4 sum = vec4(0.0);

    sum += decodeHDR(texture2D(colorTex, vec2(max(coord.x - 4.0*off, 0.0), coord.y))) * 0.05;
    sum += decodeHDR(texture2D(colorTex, vec2(max(coord.x - 3.0*off, 0.0), coord.y))) * 0.09;
    sum += decodeHDR(texture2D(colorTex, vec2(max(coord.x - 2.0*off, 0.0), coord.y))) * 0.12;
    sum += decodeHDR(texture2D(colorTex, vec2(max(coord.x - off, 0.0), coord.y))) * 0.15;
    sum += decodeHDR(texture2D(colorTex, vec2(coord.x, coord.y))) * 0.18;
    sum += decodeHDR(texture2D(colorTex, vec2(min(coord.x + off, 1.0), coord.y))) * 0.15;
    sum += decodeHDR(texture2D(colorTex, vec2(min(coord.x + 2.0*off, 1.0), coord.y))) * 0.12;
    sum += decodeHDR(texture2D(colorTex, vec2(min(coord.x + 3.0*off, 1.0), coord.y))) * 0.09;
    sum += decodeHDR(texture2D(colorTex, vec2(min(coord.x + 4.0*off, 1.0), coord.y))) * 0.05;

   gl_FragColor = encodeHDR(sum);

}

@end
@export ssr.blur_v

uniform sampler2D colorTex;
uniform sampler2D normalTex;

varying vec2 v_Texcoord;

uniform vec2 textureSize;
uniform float blurSize : 1.5;

@import qtek.util.rgbm

void main()
{
    float g = texture2D(normalTex, v_Texcoord).a;
    float off = (clamp(1.0 - g, 0.0, 1.0) + 0.3) / textureSize.y * blurSize;
    vec2 coord = v_Texcoord;

    vec4 sum = vec4(0.0);

    sum += decodeHDR(texture2D(colorTex, vec2(coord.x, max(coord.y - 4.0*off, 0.0)))) * 0.05;
    sum += decodeHDR(texture2D(colorTex, vec2(coord.x, max(coord.y - 3.0*off, 0.0)))) * 0.09;
    sum += decodeHDR(texture2D(colorTex, vec2(coord.x, max(coord.y - 2.0*off, 0.0)))) * 0.12;
    sum += decodeHDR(texture2D(colorTex, vec2(coord.x, max(coord.y - off, 0.0)))) * 0.15;
    sum += decodeHDR(texture2D(colorTex, vec2(coord.x, coord.y))) * 0.18;
    sum += decodeHDR(texture2D(colorTex, vec2(coord.x, min(coord.y + off, 1.0)))) * 0.15;
    sum += decodeHDR(texture2D(colorTex, vec2(coord.x, min(coord.y + 2.0*off, 1.0)))) * 0.12;
    sum += decodeHDR(texture2D(colorTex, vec2(coord.x, min(coord.y + 3.0*off, 1.0)))) * 0.09;
    sum += decodeHDR(texture2D(colorTex, vec2(coord.x, min(coord.y + 4.0*off, 1.0)))) * 0.05;

    gl_FragColor = encodeHDR(sum);
}

@end